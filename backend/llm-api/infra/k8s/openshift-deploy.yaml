---
apiVersion: v1
kind: Secret
metadata:
  name: llm-secrets
type: Opaque
stringData:
  API_KEYS: "dev-key-1,dev-key-2"
  RATE_LIMIT_PER_MIN: "30"

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: llm-models-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi 

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-init-script
data:
  entrypoint.sh: |
    #!/bin/bash
    set -e
    echo "üî¥ Starting Ollama server..."
    /bin/ollama serve &
    PID=$!
    echo "üü° Waiting for Ollama API..."
    until ollama list > /dev/null 2>&1; do sleep 2; done
    echo "üü¢ Ollama is up!"
    
    # NOTE: Using a tiny model for Sandbox (7GB limit prevention)
    MODEL_NAME="tinyllama" 
    
    if ! ollama list | grep -q "${MODEL_NAME}"; then
        echo "‚ö†Ô∏è Model ${MODEL_NAME} not found. Pulling..."
        ollama pull ${MODEL_NAME}
        echo "‚úÖ Model pulled."
    else
        echo "‚úÖ Model ${MODEL_NAME} exists."
    fi
    wait $PID

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-gateway
  labels:
    app: llm-saas
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llm-saas
  template:
    metadata:
      labels:
        app: llm-saas
    spec:
      hostAliases:
        - ip: "127.0.0.1"
          hostnames:
            - "ollama"

      containers:
        # 1. API Gateway (Your Code)
        - name: llm-api
          image: quay.io/c4737/llm-api:v1
          imagePullPolicy: Always
          ports:
            - containerPort: 8000
          env:
            - name: OLLAMA_HOST
              value: "localhost"
          envFrom:
            - secretRef:
                name: llm-secrets
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop: ["ALL"]
            allowPrivilegeEscalation: false
          volumeMounts:
            - name: tmp-volume
              mountPath: /tmp
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8000
            initialDelaySeconds: 15
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8000
            initialDelaySeconds: 15
            periodSeconds: 10

        # 2. Ollama Backend (Sidecar)
        - name: ollama
          image: docker.io/ollama/ollama:latest
          command: ["/bin/bash", "/scripts/entrypoint.sh"]
          ports:
            - containerPort: 11434
          env:
            - name: HOME
              value: "/tmp"

          volumeMounts:
            - name: model-storage
              mountPath: /tmp/.ollama
            - name: init-script
              mountPath: /scripts

          securityContext:
            allowPrivilegeEscalation: false

      volumes:
        - name: tmp-volume
          emptyDir: {}
        - name: model-storage
          persistentVolumeClaim:
            claimName: llm-models-pvc
        - name: init-script
          configMap:
            name: ollama-init-script
            defaultMode: 0755

---
apiVersion: v1
kind: Service
metadata:
  name: llm-service
spec:
  selector:
    app: llm-saas
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000

---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llm-route
spec:
  to:
    kind: Service
    name: llm-service
  port:
    targetPort: 8000
  tls:
    termination: edge
